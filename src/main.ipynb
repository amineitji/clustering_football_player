{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Clustering Top 5 football players**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récuparation de tous les liens des équipes du top 5 européen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeamScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def scrape_teams(self):\n",
    "        response = requests.get(self.url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        team_table = soup.find('table', {'id': 'big5_table'})\n",
    "        teams_data = []\n",
    "\n",
    "        for row in team_table.find('tbody').find_all('tr'):\n",
    "            team_cell = row.find('td', {'data-stat': 'team'})\n",
    "            if team_cell:\n",
    "                team_name = team_cell.get_text(strip=True)\n",
    "                team_link = 'https://fbref.com' + team_cell.find('a')['href'] + '/'\n",
    "                teams_data.append([team_name, team_link])\n",
    "\n",
    "        # Save team data to CSV\n",
    "        with open('../data/teams_data.csv', mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Team Name', 'Team URL'])\n",
    "            writer.writerows(teams_data)\n",
    "\n",
    "        print(\"Teams data saved to teams_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teams data saved to teams_data.csv\n"
     ]
    }
   ],
   "source": [
    "team_scraper = TeamScraper('https://fbref.com/en/comps/Big5/Big-5-European-Leagues-Stats')\n",
    "team_scraper.scrape_teams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récuparation de tous les liens des joueurs de chaque équipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayerScraper:\n",
    "    def __init__(self, teams_file):\n",
    "        self.teams_file = teams_file\n",
    "\n",
    "    def scrape_players(self):\n",
    "        # Open the output CSV file in append mode (use mode='a')\n",
    "        output_file = '../data/players_data.csv'\n",
    "        \n",
    "        # Open CSV file once outside the player loop\n",
    "        with open(output_file, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Team Name', 'Player Name', 'Position', 'Player URL'])  # Write header once\n",
    "\n",
    "            # Open the teams file and iterate through teams\n",
    "            with open(self.teams_file, newline='') as csvfile:\n",
    "                reader = csv.reader(csvfile)\n",
    "                next(reader)  # Skip header row\n",
    "                \n",
    "                # Loop through each team\n",
    "                for row in reader:\n",
    "                    team_name, team_url = row\n",
    "                    print(f\"Scraping players from {team_name}...\")\n",
    "\n",
    "                    # Fetch the page content and parse it\n",
    "                    response = requests.get(team_url)\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                    # Find the player table\n",
    "                    player_table = soup.find('table', {'class': 'stats_table sortable min_width'})\n",
    "\n",
    "                    # Iterate through all player rows\n",
    "                    for player_row in player_table.find('tbody').find_all('tr'):\n",
    "                        print(player_row)  # For debugging purposes\n",
    "                        \n",
    "                        # Extract player details\n",
    "                        player_name = player_row.find('td', {'data-stat': 'player'}).get_text(strip=True)\n",
    "                        player_position = player_row.find('td', {'data-stat': 'position'}).get_text(strip=True)\n",
    "                        player_link = 'https://fbref.com' + player_row.find('td', {'data-stat': 'player'}).find('a')['href'] + '/'\n",
    "                        \n",
    "                        # Write player data to the CSV file immediately\n",
    "                        writer.writerow([team_name, player_name, player_position, player_link])\n",
    "\n",
    "                    # Add a delay after each request to avoid being blocked (5 requests per minute = 12 seconds delay)\n",
    "                    time.sleep(12)\n",
    "\n",
    "        print(f\"Players data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping players from Monaco...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m player_scraper \u001b[38;5;241m=\u001b[39m PlayerScraper(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/teams_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplayer_scraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape_players\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 32\u001b[0m, in \u001b[0;36mPlayerScraper.scrape_players\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m player_table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstats_table sortable min_width\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Iterate through all player rows\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m player_row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mplayer_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(player_row)  \u001b[38;5;66;03m# For debugging purposes\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Extract player details\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "player_scraper = PlayerScraper('../data/teams_data.csv')\n",
    "player_scraper.scrape_players()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
